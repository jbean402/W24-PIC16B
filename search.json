[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Justine Constantino",
    "section": "",
    "text": "This is Justine Constantino‚Äôs Quarto blog for the PIC16B Winter quarter!\n‚òÜ:.ÔΩ°.o(‚âß‚ñΩ‚â¶)o.ÔΩ°.:‚òÜ"
  },
  {
    "objectID": "posts/TMDB Scraper/index.html",
    "href": "posts/TMDB Scraper/index.html",
    "title": "HW0: Palmer Penguins",
    "section": "",
    "text": "``` #### üêß Step 2: TBD\n\n\n\n\n\n\nAs a result, I got this table!\n\n\n\n‚≠êÔ∏è TBD"
  },
  {
    "objectID": "posts/TMDB Scraper/index.html#task-create-a-web-scraper-to-answer-the-following-question-what-movie-or-tv-shows-share-actors-with-your-favorite-movie-or-show.",
    "href": "posts/TMDB Scraper/index.html#task-create-a-web-scraper-to-answer-the-following-question-what-movie-or-tv-shows-share-actors-with-your-favorite-movie-or-show.",
    "title": "HW0: Palmer Penguins",
    "section": "",
    "text": "``` #### üêß Step 2: TBD\n\n\n\n\n\n\nAs a result, I got this table!\n\n\n\n‚≠êÔ∏è TBD"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn‚Äôt specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/Palmer Penguins/index.html",
    "href": "posts/Palmer Penguins/index.html",
    "title": "HW0: Palmer Penguins",
    "section": "",
    "text": "üõë BEFORE STARTING üõë Make sure to import these libraries used: Plotly, Pandas, Numpy, Seaborn\n\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/pic16b-ucla/24W/main/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)\n\n\n\npenguins.info()\n\n# cleaning the data\npenguins.isnull().sum()\n\n# filling nan values with 0\npenguins.fillna(0)\n\n\n\n.info()\n\nPresents the information of each column and the amount of entries within each column.\n\n.isnull().sum()\n\nTakes the sum of each column that has a NaN value.\n\n\n\n\n\n\n‚ÄúPlotly‚Äôs Python graphing library makes interactive, publication-quality graphs. Examples of how to make line plots, scatter plots, area charts, bar charts, error bars, box plots, histograms, heatmaps, subplots, multiple-axes, polar charts, and bubble charts.‚Äù ‚Äì From Plotly Website\n‚òÉÔ∏è I decided to use plotly because of its ability to create complex but informational graphs, charts, plots easily. I utilized the scatterplot code from their documentation to create the plot.\nfrom plotly import express as px\n\n# create a scatter plot\nfig = px.scatter(data_frame = penguins, x = 'Body Mass (g)', y = 'Flipper Length (mm)', color = \"Species\",\n                 width = 1000, height = 600, title = \"Flipper Length (mm) and Body Mass (g) for Penguin Species\")\n\nfig.update_layout(margin={\"r\":0, \"t\":50, \"l\":0, \"b\":0})\n\n#show the plot\nfig.show()\n‚òÉÔ∏è I chose to represent the flipper length (mm) and body mass (g) for all penguin species that were present in the data set.\n\n\n\nAs a result, I got this table!\n\n\n\nThis is the plot I made!\n\n\n\n\n\n‚≠êÔ∏è It was a good introduction to writing Quarto blogs. It took me a while to understand, but I managed to do it so I think that is what mattered the most. :&gt;\n‚≠êÔ∏è I want to work towards learning to read and search through documentation. In order to create the plots, I had to outsource to the Plotly documentation website, which required some reading and attention."
  },
  {
    "objectID": "posts/Palmer Penguins/index.html#task-write-a-tutorial-explaining-how-to-construct-an-interesting-data-visualization-of-the-palmer-penguins-data-set.",
    "href": "posts/Palmer Penguins/index.html#task-write-a-tutorial-explaining-how-to-construct-an-interesting-data-visualization-of-the-palmer-penguins-data-set.",
    "title": "HW0: Palmer Penguins",
    "section": "",
    "text": "üõë BEFORE STARTING üõë Make sure to import these libraries used: Plotly, Pandas, Numpy, Seaborn\n\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/pic16b-ucla/24W/main/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)\n\n\n\npenguins.info()\n\n# cleaning the data\npenguins.isnull().sum()\n\n# filling nan values with 0\npenguins.fillna(0)\n\n\n\n.info()\n\nPresents the information of each column and the amount of entries within each column.\n\n.isnull().sum()\n\nTakes the sum of each column that has a NaN value.\n\n\n\n\n\n\n‚ÄúPlotly‚Äôs Python graphing library makes interactive, publication-quality graphs. Examples of how to make line plots, scatter plots, area charts, bar charts, error bars, box plots, histograms, heatmaps, subplots, multiple-axes, polar charts, and bubble charts.‚Äù ‚Äì From Plotly Website\n‚òÉÔ∏è I decided to use plotly because of its ability to create complex but informational graphs, charts, plots easily. I utilized the scatterplot code from their documentation to create the plot.\nfrom plotly import express as px\n\n# create a scatter plot\nfig = px.scatter(data_frame = penguins, x = 'Body Mass (g)', y = 'Flipper Length (mm)', color = \"Species\",\n                 width = 1000, height = 600, title = \"Flipper Length (mm) and Body Mass (g) for Penguin Species\")\n\nfig.update_layout(margin={\"r\":0, \"t\":50, \"l\":0, \"b\":0})\n\n#show the plot\nfig.show()\n‚òÉÔ∏è I chose to represent the flipper length (mm) and body mass (g) for all penguin species that were present in the data set.\n\n\n\nAs a result, I got this table!\n\n\n\nThis is the plot I made!\n\n\n\n\n\n‚≠êÔ∏è It was a good introduction to writing Quarto blogs. It took me a while to understand, but I managed to do it so I think that is what mattered the most. :&gt;\n‚≠êÔ∏è I want to work towards learning to read and search through documentation. In order to create the plots, I had to outsource to the Plotly documentation website, which required some reading and attention."
  },
  {
    "objectID": "posts/HW 1 /index.html#create-a-database",
    "href": "posts/HW 1 /index.html#create-a-database",
    "title": "HW 1: Climate Data Visualization",
    "section": "1. Create a Database",
    "text": "1. Create a Database\nUsing the instructions from the homework, I first created a database with three tables: temperatures, stations, and countries.\nTo create a database from the NOAA climate data, I used sqlite3 to create the database and to query the data, as well as pandas to manipulate the dataframes.\n# importing the libraries  \nimport sqlite3 \nimport pandas as pd\nimport numpy as np \n\n# Importing the csv file \ntemps_df = pd.read_csv(\"temps.csv\")\ntemps_df.head()\nNext, the temperature database had to be cleaned. The prepare_df reorganizes the tempperature data so that it can be easily used.\ndef prepare_df(df):\n    df = df.set_index(keys=[\"ID\", \"Year\"])\n    df = df.stack()\n    df = df.reset_index()\n    df = df.rename(columns = {\"level_2\"  : \"Month\" , 0 : \"Temp\"})\n    df[\"Month\"] = df[\"Month\"].str[5:].astype(int)\n    df[\"Temp\"]  = df[\"Temp\"] / 100\n    return(df)\nNow, we are going to start creating the connection into the climate database. As per the homework, our database will be called climate-database.db.\nconn = sqlite3.connect(\"climate-database.db\") # temperature database\n\n\n\n\n\n\nNote\n\n\n\nWe realize that the temerature data set contains many rows, so it is better to have the data be loaded into the database in chunks. The following loop iterates through the 100000 rows of the data at a time, cleaning it with prepare_df and adding it to the database.\n\n\ntemps_iter = pd.read_csv(\"temps.csv\", chunksize = 100000)\n\nfor i, temps_df in enumerate(temps_iter): \n    df = prepare_df(temps_df)\n    df.to_sql(\"temperatures\", conn, if_exists=\"replace\" if i == 0 else \"append\", index = False)\nSince the temerature table is ready, we will start to read in the data from the station and country and add them as individual tables into the database.\n# Adding the stations table\nstations = pd.read_csv(\"station-metadata.csv\") \nstations.to_sql(\"stations\", conn, if_exists = \"replace\", index = False)\n\n# Adding the countries table\ncountries = pd.read_csv(\"countries.csv\")\ncountries.to_sql(\"countries\", conn, if_exists = \"replace\", index = False)\nTo make sure that the tables are created, we will use a cursor to look into the SQL table.\ncursor = conn.cursor() \n\ncursor.execute(\"SELECT sql FROM sqlite_master WHERE type='table';\")\n\nfor result in cursor.fetchall():\n    print(result[0])\nYou should get something like this:\n\n\n\nOutput\n\n\nOnce you have this, you have successfully created a climate database."
  },
  {
    "objectID": "posts/HW 1 /index.html#writing-a-query",
    "href": "posts/HW 1 /index.html#writing-a-query",
    "title": "HW 1: Climate Data Visualization",
    "section": "2. Writing a Query",
    "text": "2. Writing a Query\nIn order for us to access the climate-database, we need to write a SQL query. I used the following query to access key information that the homework is asking us to retrieve:\nimport sqlite3\nimport pandas as pd \n\ndef query_climate_database(db_file, country, year_begin, year_end, month): \n    \n    conn = sqlite3.connect(db_file)\n\n    query = f'''\n                SELECT S.name, S.latitude, S.longitude, C.name, T.year, T.month, T.temp\n                FROM temperatures T \n                LEFT JOIN stations S on T.id = S.id\n                LEFT JOIN countries C on SUBSTRING(T.id, 1, 2) = C.'FIPS 10-4'\n                WHERE T.year &gt;= {year_begin} AND T.year &lt;= {year_end} AND T.month == {month} AND C.name == \"{country}\"\n                '''\n\n    df = pd.read_sql_query(query, conn)\n\n    conn.close()\n    return df \nTo keep it short, here is a quick run down about what this query is doing:\nSELECT - Selecting the name, latitude, and longitude from STATIONS; Name from COUNTRIES; Year, Month, and Temp from TEMPERATURES\nFROM - Temperatures table (Aliased as T for readability purposes) ¬†\nLEFT JOIN - IDs from Temperature that equal to the FIPS 10-4 values are to be selected\nLEFT JOIN - Values from Stations that match IDs with values in the Temperatures table\nWHERE - The year is greater than or equal to year_begin and less than or equal to the year_end and where Month equals month and Country equals country. ¬†\nAfter this, you must import the .py file into the index.ipynb file, and you can start making the queries. This is a sample query provided in the homework.\nindia_df = query_climate_database(db_file = \"climate_database.db\",\n                       country = \"India\", \n                       year_begin = 1980, \n                       year_end = 2020,\n                       month = 1)"
  },
  {
    "objectID": "posts/HW 1 /index.html#creating-a-data-visualization",
    "href": "posts/HW 1 /index.html#creating-a-data-visualization",
    "title": "HW 1: Climate Data Visualization",
    "section": "3. Creating a data visualization",
    "text": "3. Creating a data visualization\nFor the assignment, we are required to produce a temperature coefficient plot that reflects an estimate of the yearly change in temperature during the specified month and time period at that station. ¬† As advised, we are going to compute the first coefficient of a linear regression at the station using code from the lecture. ¬† In order to do this, we first must import the necessary packages.\nimport plotly.express as px\nfrom sklearn.linear_model import LinearRegression\nimport datetime\nHere is the temperature_coefficient_plot and coef function provided from the lecture:\ndef coef(data_group):\n    x = data_group[[\"Year\"]] # 2 brackets because X should be a df\n    y = data_group[\"Temp\"]   # 1 bracket because y should be a series\n    LR = LinearRegression()\n    LR.fit(x, y)\n    return LR.coef_[0]\n    \ndef temperature_coefficient_plot(db_file, country, year_begin, year_end, month, min_obs, **kwargs): \n    '''\n    The output of this function should be an interactive geographic scatterplot, \n    constructed using Plotly Express, with a point for each station, such that the \n    color of the point reflects an estimate of the yearly change in temperature during \n    the specified month and time period at that station. \n    A reasonable way to do this is to compute the first coefficient of a linear regression model \n    at that station, as illustrated in the lecture where we used the .apply() method.\n    '''\n    \n    # creating the dataframe\n    df = query_climate_database(db_file, country, year_begin, year_end, month) \n    \n    # Cleaning the dataframe \n    counts = df.groupby([\"NAME\", \"Month\"])[\"Year\"].transform(len)\n    df = df[counts &gt;= min_obs]\n    coefs = df.groupby([\"NAME\", \"Month\", \"LATITUDE\", \"LONGITUDE\"]).apply(coef) #find the estimated yearly change in temperature for each station\n    coefs = coefs.round(3) # round data to 3 decimal places\n    coefs = coefs.reset_index()\n    coefs = coefs.rename(columns = {0 : \"Estimated Yearly Change (C)\"})\n    \n    title = \"Estimates of Yearly Increase in Temperature in {a} for stations in {b}, years {c} - {d}\"\\\n    .format(a=datetime.date(2021, month, 1).strftime('%B'), b=country, c=year_begin, d=year_end)\n    fig = px.scatter_mapbox(coefs,\n                            lat = \"LATITUDE\",\n                            lon = \"LONGITUDE\",\n                            hover_name = \"NAME\",\n                            color = \"Estimated Yearly Change (C)\",\n                            title = title,\n                            **kwargs)\n    return fig\n    \nNow running it, you should see this!"
  },
  {
    "objectID": "posts/HW 1 /index.html#writing-another-query",
    "href": "posts/HW 1 /index.html#writing-another-query",
    "title": "HW 1: Climate Data Visualization",
    "section": "4. Writing Another Query",
    "text": "4. Writing Another Query\nIn order to do this, I am going to write another query in the climate-database.py file. This function will collect the Station name and ID, Country name, and the Temperature year, month, and temperature from the climate-database.\ndef second_climate_database(db_file, country1, country2, year_begin, year_end): \n    \n    conn = sqlite3.connect(db_file) \n    \n    query = f'''\n            SELECT S.ID, S.name, C.name, T.year, T.month, T.temp\n            FROM temperatures T \n            LEFT JOIN stations S on T.id = S.id\n            LEFT JOIN countries C on SUBSTRING(T.id, 1, 2) = C.'FIPS 10-4' \n            \n            WHERE T.year &gt;= {year_begin} AND T.year &lt;= {year_end} AND C.name == \"{country1}\"\n            UNION \n            SELECT S.ID, S.name, C.name, T.year, T.month, T.temp\n            FROM temperatures T \n            LEFT JOIN stations S on T.id = S.id\n            LEFT JOIN countries C on SUBSTRING(T.id, 1, 2) = C.'FIPS 10-4' \n            \n            WHERE T.year &gt;= {year_begin} AND T.year &lt;= {year_end} AND C.name == \"{country2}\"\n            \n            '''\n    \n    df = pd.read_sql_query(query, conn) \n    \n    conn.close() \n    return df \nCreating a data frame of the data, this is what you should expect to get.\ndf = second_climate_database(db_file = \"climate_database.db\", \n                           country1 = \"Philippines\", \n                           country2 = \"Indonesia\",\n                           year_begin = 1970, \n                           year_end = 2020)\ndf\n\nNow onto creating some visualizations from this data set!\n\nComparing the Minimum and Maximum Temperatures from two countries\nIn order to do this, we are going to look at two different countries over the span of 70 years. Using the function call from earlier, we get data from both the Philippines and Indonesia. ¬†\nWe will use the groupby and max() function in order to find both the max and min values per country as follows:\nprint(\"Maximum Temp per Year\")\nmax_data = df.groupby(['Name', 'NAME', 'Year'])['Temp'].max()\nprint(max_data)\n\nprint()\nprint(\"Minimum Temp per Year\")\nmin_data = df.groupby(['Name', 'NAME', 'Year'])['Temp'].min()\nprint(min_data)\n\n# turning them into dataframes \nmin_data_df = min_data.to_frame() \ndisplay(min_data_df)\n\nmax_data_df = max_data.to_frame() \ndisplay(max_data_df)\nThis organizes the data into two seperate dataframes: min_data_df and max_data_df. We will use this in order to calculate the means next.\nprint(\"Min temp mean\")\n# min data \nmin_mean = min_data_df.groupby(['Year', 'Name'])['Temp'].mean()\nprint(min_mean)\n\nprint()\nprint(\"Max temp mean\")\n# max data \nmax_mean = max_data_df.groupby(['Year', 'Name'])['Temp'].mean()\nprint(max_mean)\nNow that this is done, we must convert the datasets into dataframes. Currently, they are series data, therefore we must use the to_frame() and reset_index() in order to do this.\n# converting series to frame \nmin_df = min_mean.to_frame()\nmax_df = max_mean.to_frame()\n\nmin_df = min_df.reset_index()\nmin_df\n\nmax_df = max_df.reset_index() \nmax_df\nNow we can start putting it on a plot! We will create two seperate plots displaying the minimum and maximum temperatures of the countries.\nfig1 = px.line(min_df, x=\"Year\", y=\"Min_Temp\", color=\"Name\", title=\"Comparing the Min Temperatures of the Philippines and Indonesia\")\nfig2 = px.line(max_df, x = \"Year\", y = \"Temp\", color=\"Name\", title=\"Comparing the Max Temperatures of the Philippines and Indonesia\")\n\nfig1.show()\nfig2.show()\n\n\n\nVisualization 2: Comparing the Average Temperatures of Two Different Airports in the Philippines\nSimilar to the first visualization, we first call on the SQL query function for this data.\ndf = second_climate_database(db_file = \"climate_database.db\", \n                           country1 = \"Philippines\", \n                           country2 = \"United States\",\n                           year_begin = 2000, \n                           year_end = 2010)\ndf\nNow, since we are only looking at the Philippines, then we must drop the United States data from the dataframe.\n# filtering the united states data out since we only want to focus on the philippines\ndf_filtered = df[df['Name'] != 'United States']\ndf_filtered\nLet‚Äôs look at the unique values in the NAME column to find the airports.\n# now looking at the unique values \nunique_values = df['NAME'].unique()\nunique_values\n\nWe are now going to assign specific airports and start filtering the dataframe so that we only get the values associated with those two airports.\nspecific_airports = ['MANILA_INT_AIRPORT', 'MACTAN_CEBU_INTL'] \n\ndf_filtered = df_filtered[df_filtered['NAME'].isin(specific_airports)]\nprint(df_filtered)\nIn order to compare both, I figured a box plot was most appropriate since this displays the average temperatures for the two airports between 2000 and 2008. Here is what the visualization looks like after running the code:\nfig = px.box(df_filtered, x='Year', y='Temp', color='NAME')\nfig.show()"
  },
  {
    "objectID": "posts/HW 1 /index.html#takeaways",
    "href": "posts/HW 1 /index.html#takeaways",
    "title": "HW 1: Climate Data Visualization",
    "section": "Takeaways",
    "text": "Takeaways\n\nThis assignment was long overdue since I felt very stuck trying to figure out how to do the SQL queries. However, after spending time on them, I slowly started to understand and get the hang of writing them. I thought that it was very helpful to just think about it in a logical way and verbalize them (i.e.¬†I am going to SELECT country names and temperatures FROM the temperature tables WHERE the country names match x.)\n\nI also learned how important data cleaning was! There were many times where my data visualizations did not look correct because I did not clean and organize the dataframes.\n\nThank you for reading on how to create data visualizations!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Winter PIC16A",
    "section": "",
    "text": "HW0: Palmer Penguins\n\n\n\n\n\n\nweek 1\n\n\nHomework 0\n\n\n\n\n\n\n\n\n\nOct 1, 2025\n\n\nJustine Constantino\n\n\n\n\n\n\n\n\n\n\n\n\nHW0: Palmer Penguins\n\n\n\n\n\n\nweek 1\n\n\nHomework 0\n\n\n\n\n\n\n\n\n\nOct 1, 2025\n\n\nJustine Constantino\n\n\n\n\n\n\n\n\n\n\n\n\nHW 1: Climate Data Visualization\n\n\n\n\n\n\nweek 1\n\n\nHomework 1\n\n\n\n\n\n\n\n\n\nMar 11, 2024\n\n\nJustine Constantino\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJan 11, 2024\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJan 8, 2024\n\n\nTristan O‚ÄôMalley\n\n\n\n\n\n\nNo matching items"
  }
]